%% LyX 2.3.4.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsthm}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}
\theoremstyle{definition}
\newtheorem{problem}[thm]{\protect\problemname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{pgfplots}
\usepackage{enumitem}
\setenumerate{label=A)}
\setenumerate[2]{label=a)}

\makeatother

\usepackage{babel}
\providecommand{\problemname}{Problem}
\providecommand{\theoremname}{Theorem}

\begin{document}

\subsection{Teorema de De Finetti}

Las urnas de Polya son un buen ejemplo de variables aleatorias intercambiables

\section{Primeros ejemplos de medida de De Finetti}

A pesar de que el teorema de representaci\'{o}n de De Finetti nos
garantizan la existencia de una distribuci\'{o}n a priori no nos dice
cual es el precisamente, podemos recolectar algunos ejemplos de ciertas
bibliograf\'{\i}as para hacernos una idea de como operar con estos
conceptos. Si tenemos una muestra aleatoria independiente es simple
ver que
\[
f\left(x_{1},\ldots,x_{n}\right)=\prod_{i=1}^{n}f\left(x_{i}\right)\implies f\left(x_{n+1},\ldots,x_{n+m}|x_{1},\ldots,x_{n}\right)=\prod_{i=n+1}^{m}f\left(x_{i}\right)
\]
, esto quiere decir que no hay aprendizaje desde la experiencia. \marginpar{no hay aprendizaje desde la experiencia} 
\begin{thm}
Sea $\theta\sim f\left(\theta\right)$ y $Y_{1},\ldots,Y_{n}$ condicionalmente
independientes de $\theta$ entonces $Y_{1},\ldots,Y_{n}$ es intercambiable.
\end{thm}

\begin{proof}
Basta con probar lo siguiente 
\begin{align*}
f\left(y_{1},\ldots,y_{n}\right) & =\int f\left(y_{1},\ldots,y_{n}|\theta\right)dF\left(\theta\right)\\
 & =\int\prod_{i=1}^{n}f\left(y_{i}|\theta\right)dF\left(\theta\right)\\
 & =f\left(y_{\pi_{1}},\ldots,y_{\pi_{n}}\right)
\end{align*}
\end{proof}
B\'{a}sicamente el teorema de De Finetti es el rec\'{\i}proco de esta
proposici\'{o}n. La prueba de De Finetti para variables aleatorias
dicot\'{o}micas se explica a continuaci\'{o}n. Consideremos el caso
de una secuencia intercambiable de variables aleatorias $0-1$. Entonces
\begin{align*}
p\left(y_{1},\ldots,y_{n}\right) & =\int\prod_{i=1}^{n}p\left(y_{i}|\theta\right)dF\left(\theta\right)
\end{align*}
donde 
\[
F\left(\theta\right)=\lim_{n\to\infty}P\left(s_{n}\leq\theta\right)
\]
notemos que 
\[
p\left(y_{1}+\ldots+y_{n}=s_{n}\right)=\binom{n}{s_{n}}p\left(y_{1},\ldots,y_{n}\right)=\binom{n}{s_{n}}p\left(y_{\pi_{1}},\ldots,y_{\pi_{n}}\right)
\]
dado que vale para cualquier permutaci\'{o}n, ahora bien, sea $N>n$
entonces 
\[
p\left(y_{1}+\ldots+y_{n}=s_{n}\right)=\sum p\left(y_{1}+\ldots+y_{n}=s_{n}|y_{1}+\ldots+y_{N}=s_{N}\right)p\left(y_{1}+\ldots+y_{N}=s_{N}\right)
\]


\subsection{Urnas de Polya }

Las urnas de Polya son un buen ejemplo de intercambiabilidad: Tenemos
$b_{0}$ bolitas negras y $w_{0}$ bolitas blancas en una urna. Extraigo
una bolita aleatoriamente, si es blanca retorno la bolita a la urna
y agrego una blanca, si es negra retorno la bolita y agrego una negra.
$B_{n}:$ El n\'{u}mero de bolas negras dado $n$ ensayos es.
\[
B_{n}\sim Bin\left(n,\frac{b_{0}}{b_{0}+w_{0}}\right)
\]
la demostraci\'{o}n biene como sigue 
\begin{align*}
\text{Bolas negras iniciales }B_{0} & =b_{0}\\
\implies P\left(B_{0}=b_{0}\right) & =1\\
P\left(B_{1}=b_{0}+1\right) & =\frac{b_{0}}{b_{0}+w_{0}}
\end{align*}

Para $n=2$ 
\begin{align*}
P\left(B_{2}=b_{0}\right) & =\frac{w_{0}}{w_{0}+b_{0}}\cdot\frac{w_{0}+1}{w_{0}+b_{0}+1}\\
P\left(B_{2}=b_{0}+1\right) & =\frac{w_{0}}{w_{0}+b_{0}}\frac{b_{0}}{w_{0}+b_{0}+1}+\frac{b_{0}}{w_{0}+b_{0}}\frac{w_{0}}{w_{0}+b_{0}+1}\\
P\left(B_{2}=b_{0}+2\right) & =\frac{b_{0}}{w_{0}+b_{0}}\cdot\frac{b_{0}+1}{w_{0}+b_{0}+1}
\end{align*}
debemos demostrar que la secuencia de variables aleatorias $X_{1},\ldots,X_{n}$
de la secuencua de colores en cada una de las $n$ extracciones en
la urna es una secuencia de variables aleatorias intercambiables.
Sea $Y_{n}$ la proporci\'{o}n de bolas negras en la urna en dado
$n$ ensayos. 
\[
Y_{n}=\frac{B_{n}}{B_{n}+W_{n}}=\frac{B_{n}}{b_{0}+w_{0}+n}
\]
con esto podemos decir que 
\[
B_{n+1}=\begin{cases}
B_{n} & \text{con probabilidad }1-Y_{n}\\
B_{n}+1 & \text{con probabilidad }Y_{n}
\end{cases}
\]
entonces 
\begin{align*}
E\left(Y_{n+1}|X_{1},\ldots,X_{n}\right) & =E\left(\frac{B_{n+1}}{b_{0}+w_{0}+n}|X_{1},\ldots,X_{n}\right)\\
 & =\frac{1}{b_{0}+w_{0}+n}E\left(B_{n+1}|X_{1},\ldots,X_{n}\right)\\
 & =\frac{1}{b_{0}+w_{0}+n}\left(B_{n}\left(1-Y_{n}\right)+\left(B_{n}+1\right)Y_{n}\right)\\
 & =\frac{1}{b_{0}+w_{0}+n}\left(B_{n}-B_{n}Y_{n}+B_{n}Y_{n}+Y_{n}\right)\\
 & =\frac{1}{b_{0}+w_{0}+n}\left(B_{n}+Y_{n}\right)\\
 & =Y_{n}
\end{align*}


\subsection{Blackwell and MacQueen 1973}

Revisaremos la implicancias de este art\'{\i}culo en estad\'{\i}stica
bayesiana no param\'{e}trica.

\section{BDA 3}

En esta secci\'{o}n se ilustrar\'{a}n algunos conceptos \'{u}tiles
expuestos en el libro de Gelman y algunos ejercicios que son frecuentes
en c\'{a}tedras de inferencia bayesiana que se basan en el texto.

\subsection{Introducci\'{o} y motivaci\'{o}n}

\subsection{Modelos uniparam\'{e}tricos}
\begin{problem}[cap2.prob5]
La distribuci\'{o}n a posterior es un compromiso entre la priori
y los datos: Sea $n$ el n\'{u}mero de lanzamientos de una moneda,
la cual tiene probabilidad de $\theta$ de ser cara. (a) Si la distribuci\'{o}n
a priori es una uniforme entre $0,1$, derive la distribuci\'{o}n
predictiva 
\[
p\left(y=k\right)=\int_{0}^{1}p\left(y=k|\theta\right)d\theta
\]
para cada $k=1,\ldots,n$. Sol: 
\begin{align*}
p\left(y=k\right) & =\int_{0}^{1}p\left(y=k,\theta\right)d\theta\\
 & =\int_{0}^{1}p\left(y=k|\theta\right)\underbrace{p\left(\theta\right)}_{1}d\theta\\
 & =\int_{0}^{1}p\left(y=k|\theta\right)d\theta\\
 & =\int_{0}^{1}\binom{n}{k}\theta^{k}\left(1-\theta\right)^{n-k}d\theta\\
 & =\binom{n}{k}\int_{0}^{1}\theta^{k}\left(1-\theta\right)^{n-k}d\theta\\
 & =\binom{n}{k}\frac{\Gamma\left(k+1\right)\Gamma\left(n-k+1\right)}{\Gamma\left(n+2\right)}\\
 & =\binom{n}{k}\frac{k!\left(n-k\right)!}{\left(n+1\right)!}\\
 & =\frac{1}{n+1}
\end{align*}
usamos el siguiente resultado 
\begin{align*}
\int_{0}^{1}\frac{\Gamma\left(\alpha+\beta\right)}{\Gamma\left(\alpha\right)\Gamma\left(\beta\right)}\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1}d\theta & =1\\
\implies\int_{0}^{1}\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1}d\theta & =\frac{\Gamma\left(\alpha\right)\Gamma\left(\beta\right)}{\Gamma\left(\alpha+\beta\right)}\\
\implies\int_{0}^{1}\theta^{k}\left(1-\theta\right)^{n-k}d\theta & =\frac{\Gamma\left(k+1\right)\Gamma\left(n-k+1\right)}{\Gamma\left(k+1+n-k+1\right)}=\frac{\Gamma\left(k+1\right)\Gamma\left(n-k+1\right)}{\Gamma\left(n+2\right)}
\end{align*}
(b) Supona que asigna una distribuci\'{o}n $\theta\sim\text{beta}\left(\alpha,\beta\right)$
y usted observa $y$ caras en sus $n$ lanzamientos. Muestre algebraicamente
que la media posterior involucra a la media a priori y la media de
los datos. 
\begin{align*}
p\left(\theta|y\right) & \propto p\left(y|\theta\right)p\left(\theta\right)\\
 & =\binom{n}{y}\theta^{y}\left(1-\theta\right)^{n-y}\frac{\Gamma\left(\alpha+\beta\right)}{\Gamma\left(\alpha\right)\Gamma\left(\beta\right)}\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1}\\
 & \propto\theta^{\alpha+y-1}\left(1-\theta\right)^{\beta+n-y-1}\\
\implies\theta|y & \sim\text{beta}\left(\alpha+y,\beta+n-y\right)
\end{align*}
luego, la media condicional 
\begin{align*}
E\left(\theta|y\right) & =\frac{\alpha+y}{\alpha+y+\beta+n-y}\\
 & =\frac{\alpha+y}{\alpha+\beta+n}\\
 & =\frac{\alpha}{\alpha+\beta+n}+\frac{y}{\alpha+\beta+n}\\
 & =\frac{\alpha+\beta}{\alpha+\beta}\frac{\alpha}{\alpha+\beta+n}+\frac{n}{n}\frac{y}{\alpha+\beta+n}\\
 & =\frac{\alpha+\beta}{\alpha+\beta+n}\underbrace{\frac{\alpha}{\alpha+\beta}}_{E\left(\theta\right)}+\frac{n}{\alpha+\beta+n}\frac{y}{n}
\end{align*}
\end{problem}


\paragraph*{2.5c. }

Muestre que si la priori es uniforme entonces la media a posteriori,
entonces la varianza a posterior es siempre menor a la varianza a
priori. Sol: Sea $\theta\sim\text{beta}\left(1,1\right)$

\begin{align*}
\text{Var}\left(\theta|y\right) & =\frac{\left(\alpha+y\right)\left(\beta+n-y\right)}{\left(\alpha+y+\beta+n-y\right)^{2}\left(\alpha+y+\beta+n-y\right)}\\
 & =\frac{\left(\alpha+y\right)\left(\beta+n-y\right)}{\left(\alpha+\beta+n\right)^{2}\left(\alpha+\beta+n+1\right)}\\
 & =\frac{\left(1+y\right)\left(1+n-y\right)}{\left(2+n\right)^{2}\left(2+n+1\right)}=\frac{\left(1+y\right)\left(1+n-y\right)}{\left(2+n\right)^{2}\left(n+3\right)}\leq\frac{1}{12}=\text{Var}\left(\theta\right)
\end{align*}


\paragraph*{2.5d. }

De un ejemplo de una priori $\text{beta}\left(\alpha,\beta\right)$
con $y$ y $n$ donde va varianza posterior es menor a la varianza
a priori. Sol. 
\[
\frac{\left(\alpha+y\right)\left(\beta+n-y\right)}{\left(\alpha+\beta+n\right)^{2}\left(\alpha+\beta+n+1\right)}\leq\frac{\alpha\beta}{\left(\alpha+\beta\right)^{2}\left(\alpha+\beta+1\right)}
\]
y seleccionamos valores que respeten la igualdad.

\paragraph*{2.7a.}

Sobre prioris no informativas: Para la verosimilitud binomial $y\sim\text{bin}\left(n,\theta\right)$
, muestre que $p\left(\theta\right)\propto\theta^{-1}\left(1-\theta\right)^{-1}$
es la priori uniforme para el par\'{a}metro natural de la familia
exponencial. Sol. Sabemos que la funci\'{o}n de enlace can\'{o}nica
de distribuci\'{o}n binomial es $\phi=\log\left(\frac{\theta}{1-\theta}\right)=h\left(\theta\right)$.
sabiendo que 
\begin{align*}
p\left(\theta\right) & =\underbrace{p\left(\phi\right)}_{\propto1}\left|\frac{d\phi}{d\theta}\right|\\
 & \propto\left|\frac{d}{d\theta}\log\left(\frac{\theta}{1-\theta}\right)\right|\\
 & =\frac{1}{\theta}+\frac{1}{1-\theta}\\
 & =\theta^{-1}\left(1-\theta\right)^{-1}
\end{align*}
 .ver BDA (ecuaci\'{o}n 2.19) 

\paragraph*{2.7b.}

Muestre que si $y=0$ o $n=0$ la distribuci\'{o}n posterior es impropia.
Sol: Si tenemos que 
\begin{align*}
p\left(\theta|y\right) & \propto p\left(y|\theta\right)p\left(\theta\right)\\
 & =\binom{n}{y}\theta^{y}\left(1-\theta\right)^{n-y}\theta^{-1}\left(1-\theta\right)^{-1}\\
 & \propto\theta^{y-1}\left(1-\theta\right)^{n-y-1}=\begin{cases}
\theta^{-1}\left(1-\theta\right)^{n-1} & y=0\\
\theta^{-1}\left(1-\theta\right)^{-1} & n=0\\
\theta^{y-1}\left(1-\theta\right)^{n-y-1} & \text{otro caso}
\end{cases}
\end{align*}

en los dos primeros casos no integra 1

\paragraph*{2.8a. }

Distribuci\'{o}n normal con media desconocida: Sea una muestra aleatoria
de $n$ estudiantes extraidos de una poblaci\'{o}n lo suficientemente
grande para luego medir su peso. La media del peso viene dada por
$\overline{y}=150$. Asuma que pero en la poblaci\'{o}n se distribuye
normal con media desconocida $\theta$ y desviaci\'{o}n estandar de
$20$. Suponga una distribuci\'{o}n a priori para $\theta$ normal
con media $180$ y desviaci\'{o}n estandar de $40$. Encuentre la
distribuci\'{o}n a posterior de $\theta$. Sol: Solo con el objetivo
de ejercitar todos los pasos deduciremos la distribuci\'{o}n a posterior
completamente.

\begin{align*}
y|\theta & \sim N\left(\theta,\sigma^{2}\right)\\
\theta & \sim N\left(\mu_{0},\tau_{0}^{2}\right)
\end{align*}
entonces tenemos que 
\begin{align*}
p\left(\theta|y\right) & \propto p\left(y|\theta\right)p\left(\theta\right)\\
 & =\prod_{i=1}^{n}\left[\left(2\pi\sigma^{2}\right)^{-1/2}\exp\left(-\frac{1}{2\sigma^{2}}\left(y_{i}-\theta\right)^{2}\right)\right]\left(2\pi\tau_{0}^{2}\right)^{-1/2}\exp\left(-\frac{1}{2\tau_{0}^{2}}\left(\theta-\mu_{0}\right)^{2}\right)\\
 & \propto\exp\left(\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(y_{i}-\theta\right)^{2}\right)\exp\left(\frac{1}{2\tau_{0}^{2}}\left(\theta-\mu_{0}\right)^{2}\right)\\
 & =\exp\left(-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(y_{i}-\theta\right)^{2}-\frac{1}{2\tau_{0}^{2}}\left(\theta-\mu_{0}\right)^{2}\right)\\
 & \propto\exp\left(-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(y_{i}^{2}-2y_{i}\theta+\theta^{2}\right)-\frac{1}{2\tau_{0}^{2}}\left(\theta^{2}-2\theta\mu_{0}+\mu^{2}\right)\right)\\
 & \propto\exp\left(-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(\theta^{2}-2y_{i}\theta\right)-\frac{1}{2\tau_{0}^{2}}\left(\theta^{2}-2\theta\mu_{0}\right)\right)
\end{align*}

entonces 
\[
\theta|y\sim N\left(\frac{\frac{1}{\sigma^{2}}\mu_{0}+\frac{n}{\tau_{0}^{2}}\overline{y}}{\frac{1}{\sigma^{2}}+\frac{n}{\tau_{0}^{2}}},\frac{1}{\frac{1}{\sigma^{2}}+\frac{n}{\tau_{0}^{2}}}\right)
\]
para nuestro caso tenemos que 
\[
\theta|y\sim N\left(\frac{\frac{1}{\sigma^{2}}\mu_{0}+\frac{n}{\tau_{0}^{2}}\overline{y}}{\frac{1}{\sigma^{2}}+\frac{n}{\tau_{0}^{2}}},\frac{1}{\frac{1}{\sigma^{2}}+\frac{n}{\tau_{0}^{2}}}\right)
\]
en nuestro cado 
\[
\theta|y\sim N\left(\frac{\frac{1}{20^{2}}180+\frac{n}{40^{2}}150}{\frac{1}{20^{2}}+\frac{n}{40^{2}}},\frac{1}{\frac{1}{20^{2}}+\frac{n}{40^{2}}}\right)
\]


\paragraph*{2.8b.}

La forma de encontrar la distribuci\'{o}n predictiva en media esperanza
y varianza iterada. 
\[
\tilde{y}|y\sim N\left(\widetilde{\mu},\widetilde{\sigma}^{2}\right)
\]
donde 
\begin{align*}
\widetilde{\mu} & =E\left(\tilde{y}|y\right)\\
 & =E\left(E\left(\tilde{y}|\theta,y\right)|y\right)\\
 & =E\left(E\left(\tilde{y}|\theta\right)|y\right)\\
 & =E\left(\theta|y\right)\\
 & =\frac{\frac{1}{\sigma^{2}}\mu_{0}+\frac{n}{\tau_{0}^{2}}\overline{y}}{\frac{1}{\sigma^{2}}+\frac{n}{\tau_{0}^{2}}}
\end{align*}
y la varianza 
\begin{align*}
\widetilde{\sigma}^{2} & =var\left(\tilde{y}|y\right)\\
 & =var\left(E\left(\tilde{y}|\theta\right)|y\right)+E\left(var\left(\tilde{y}|\theta\right)|y\right)\\
 & =var\left(\theta|y\right)+E\left(\sigma^{2}|y\right)\\
 & =\frac{1}{\frac{1}{\sigma^{2}}+\frac{n}{\tau_{0}^{2}}}+\sigma^{2}
\end{align*}
por lo tanto 
\[
\tilde{y}|y\sim N\left(\frac{\frac{1}{20^{2}}180+\frac{n}{40^{2}}150}{\frac{1}{20^{2}}+\frac{n}{40^{2}}},\frac{1}{\frac{1}{\sigma^{2}}+\frac{n}{\tau_{0}^{2}}}+\sigma^{2}\right)
\]


\paragraph*{4.2.a}

normalidad asint\'{o}tica de la posterior: sea $y_{1}\ldots,y_{5}$
muestras independientes de una distribuci\'{o}n cauchy con parametro
de posici\'{o}n desconocido $\theta$ y escala conicda $1$. $p\left(y_{i}|\theta\right)\propto\frac{1}{1+\left(y_{i}-\theta\right)^{2}}$.
Asuma que la distribuci\'{o}n a priori para 
\[
\theta\sim\text{uniforme}\left(0,1\right)
\]
 determine la derivada y la segunda derivada del logaritmo de la verosimilitud.
sol
\[
l\left(\theta|y\right)=\prod_{i=1}^{n}\frac{1}{1+\left(y_{i}-\theta\right)^{2}}
\]


\section{Ayudant\'{\i}a}

Ejercicios de las ayudant\'{\i}as del segundo semestre del 2020

\subsection{Ayudant\'{\i}a 1}

\subsection{Ayudant\'{\i}a 2}

\subsection{Ayudant\'{\i}a 3}

\subsection{Ayudant\'{\i}a 4}

\subsection{Ayudant\'{\i}a 5}

\paragraph{4.5.1a.}

$x_{i}$ permutables $p\left(\mu,\sigma^{2}\right)\propto\left(\sigma^{2}\right)^{-3/2}$.
Buscamos la distribuci\'{o}n a posterior 
\begin{align*}
p\left(\mu,\sigma^{2}|\mathbf{x}\right) & \propto p\left(\mu,\sigma^{2}\right)p\left(\mathbf{x}|\mu,\sigma^{2}\right)\\
 & =\left(\sigma^{2}\right)^{-3/2}\sigma^{-n/2}\exp\left(-\frac{1}{2\sigma^{2}}\sum\left(x_{i}-\mu\right)^{2}\right)\\
 & =\left(\sigma^{2}\right)^{-3/2}\sigma^{-n/2}\exp\left(-\frac{1}{2\sigma^{2}}\sum\left(x_{i}-\overline{x}\right)^{2}+n\left(\overline{x}-\mu\right)^{2}\right)\\
 & =\left(\sigma^{2}\right)^{-3/2}\sigma^{-n/2}\exp\left(-\frac{1}{2\sigma^{2}}\left(\sum\left(x_{i}-\overline{x}\right)^{2}+n\left(\overline{x}-\mu\right)^{2}\right)\right)\\
 & =\left(\sigma^{2}\right)^{-3/2-n/2}\exp\left(-\frac{1}{2\sigma^{2}}\left(\left(n-1\right)s^{2}+n\left(\overline{x}-\mu\right)^{2}\right)\right)\\
\text{kerner de una normal} & \propto\exp\left(-\frac{1}{2\sigma^{2}}\left(n\left(\overline{x}-\mu\right)^{2}\right)\right)\\
\implies\mu|\mathbf{x},\sigma^{2} & \sim N\left(\overline{x},\sigma^{2}/n\right)
\end{align*}


\paragraph{4.5.1b.}

posteriori marginal

\begin{align*}
p\left(\mu,\sigma^{2}|\mathbf{x}\right) & \propto\left(\sigma^{2}\right)^{-3/2-n/2}\exp\left(-\frac{1}{2\sigma^{2}}\left(\left(n-1\right)s^{2}+n\left(\overline{x}-\mu\right)^{2}\right)\right)\\
 & =\left(\sigma^{2}\right)^{-3/2-n/2}\exp\left(-\frac{1}{2\sigma^{2}}\left(n-1\right)s^{2}\right)\underbrace{\exp\left(-\frac{1}{2\sigma^{2}}n\left(\overline{x}-\mu\right)^{2}\right)\left(\frac{\sigma}{n}\right)^{-1/2}}_{\text{kernel de una normal}}\left(\frac{\sigma}{n}\right)^{1/2}\\
\text{integrando}\\
\implies p\left(\sigma^{2}|\mathbf{x}\right) & \propto\left(\sigma^{2}\right)^{-\left(n/2+1\right)}\exp\left(-\frac{1}{2\sigma^{2}}\left(n-1\right)s^{2}\right)\\
\sigma^{2}|\mathbf{x} & \sim\chi^{2}-\mathbf{inv}\left(n,s^{2}\right)
\end{align*}


\paragraph{4.5.1c.}

calcular la distribuci\'{o}n a posteriori marginal de $\mu$
\begin{align*}
p\left(\mu|\mathbf{x}\right) & =\int_{0}^{\infty}p\left(\mu,\sigma^{2}|\mathbf{x}\right)d\sigma^{2}\\
 & \propto\int_{0}^{\infty}\left(\sigma^{2}\right)^{-3/2}\sigma^{-n/2}\exp\left(-\frac{1}{2\sigma^{2}}A\right)d\sigma^{2}\\
A & =\sum\left(x_{i}-\overline{x}\right)^{2}+n\left(\overline{x}-\mu\right)^{2}
\end{align*}
con el siguiente cambio de variable $z=A/\left(2\sigma^{2}\right)$,
$\text{\ensuremath{\sigma^{2}=A/2z} }$ y $d\sigma^{2}=-\frac{A}{2z^{2}}dz$.
Luego 
\begin{align*}
p\left(\mu|\mathbf{x}\right) & \propto\int_{0}^{\infty}\left(\frac{A}{2z}\right)^{-3/2-n/2}\exp\left(-z\right)\left(-\frac{A}{2z^{2}}\right)dz\\
 & \propto\left(\frac{A}{2z}\right)^{-1/2-n/2}
\end{align*}


\paragraph{4.5.1d.}

Utilice los resultados anteriores para encontrar la distribuci\'{o}n
predictiva a posteriori usando el m\'{e}todo de la composici\'{o}n. 
\begin{enumerate}
\item para $s=1\ldots,S$
\item genera $\mu^{\left(s\right)}\sim p\left(\mu|\mathbf{y}\right)$
\item genera $\sigma^{2\left(s\right)}\sim p\left(\sigma^{2}|\mathbf{y}\right)$
\item $y^{s}\sim p\left(x|\mu^{\left(s\right)},\sigma^{2\left(s\right)}\right)$
\end{enumerate}

\paragraph{4.5.1e}

Repita a) y c) con la priori informativa: $p\left(\mu,\sigma^{2}\right)=p\left(\mu|\sigma^{2}\right)p\left(\sigma^{2}\right)$.
Luego 
\begin{align*}
p\left(\mu,\sigma^{2}|\mathbf{x}\right) & \propto p\left(\mu,\sigma^{2}\right)p\left(\mathbf{x}|\mu,\sigma^{2}\right)\\
 & =p\left(\mu|\sigma^{2}\right)p\left(\sigma^{2}\right)p\left(\mathbf{x}|\mu,\sigma^{2}\right)\\
 & =
\end{align*}


\paragraph{4.5.2a.}

Encontre la distribuci\'{o}n a posterior de $\beta,\sigma^{2}$. Conocido
que la distribuci\'{o}n a priori de $p\left(\beta,\sigma^{2}\right)\propto\sigma^{-2}$
\begin{align*}
y|\beta,\sigma^{2},X & \sim N\left(X\beta,\sigma^{2}I\right)\\
p\left(y|\beta,\sigma^{2},X\right) & \propto\det\left(\sigma^{2}I\right)^{-1/2}\exp\left(-\frac{1}{2\sigma^{2}}\left(y-X\beta\right)^{t}\left(y-X\beta\right)\right)\\
 & =\left(\sigma^{2}\right)^{-1/2}\exp\left(-\frac{1}{2\sigma^{2}}\left(y-X\beta\right)^{t}\left(y-X\beta\right)\right)\\
 & =\\
\implies p\left(\beta,\sigma^{2}|y,X\right) & \propto\left(\sigma^{2}\right)^{-n/2}\exp\left(-\frac{1}{2\sigma^{2}}\left(y-X\beta\right)^{t}\left(y-X\beta\right)\right)\sigma^{-2}\\
 & =\left(\sigma^{2}\right)^{-n/2-2}\exp\left(-\frac{1}{2\sigma^{2}}\left(y-X\beta\right)^{t}\left(y-X\beta\right)\right)
\end{align*}


\paragraph*{4.5.2b.}

Demuestre que la distribuci\'{o}n a posterior de \textbf{$\beta$
}dado $\sigma^{2}$ corresponde a una distribuci\'{o}n normal\textbf{
$\hat{\beta},\sigma^{2}\left(X^{t}X\right)^{-1}$. }Sol: Sabemos que
\begin{align*}
p\left(\beta,\sigma^{2}|y\right) & =p\left(\sigma^{2}|\beta,y\right)p\left(\beta|y\right) & \implies & p\left(\beta|y\right)=\frac{p\left(\beta,\sigma^{2}|y\right)}{p\left(\sigma^{2}|\beta,y\right)}=\frac{p\left(y|\beta,\sigma^{2}\right)p\left(\beta,\sigma^{2}\right)}{p\left(\sigma^{2}|\beta,y\right)}\\
p\left(\beta,\sigma^{2}|y\right) & =p\left(\beta|\sigma^{2},y\right)p\left(\sigma^{2}|y\right) & \implies & p\left(\sigma^{2}|y\right)=\frac{p\left(y|\beta,\sigma^{2}\right)p\left(\beta,\sigma^{2}\right)}{p\left(\beta|\sigma^{2},y\right)}
\end{align*}
El camino quiz\'{a}s es m\'{a}s simple si hacemos la siguiente operatoria:

\begin{align*}
p\left(\beta|\sigma^{2},y\right) & =\frac{p\left(\beta,\sigma^{2}|,y\right)}{p\left(\sigma^{2}|y\right)}\\
 & \propto p\left(\beta,\sigma^{2}|,y\right)\\
 & =\left(\sigma^{2}\right)^{-n/2-2}\exp\left(-\frac{1}{2\sigma^{2}}\left(y-X\beta\right)^{t}\left(y-X\beta\right)\right)\\
 & \propto\exp\left(-\frac{1}{2\sigma^{2}}\left(y-X\beta\right)^{t}\left(y-X\beta\right)\right)\\
 & =\exp\left(-\frac{1}{2\sigma^{2}}\left(y-X\hat{\beta}\right)^{t}\left(y-X\hat{\beta}\right)-\frac{1}{2\sigma^{2}}\left(\beta-\hat{\beta}\right)^{t}X^{t}X\left(\beta-\hat{\beta}\right)\right)\\
 & \propto\exp\left(-\frac{1}{2\sigma^{2}}\left(\beta-\hat{\beta}\right)^{t}X^{t}X\left(\beta-\hat{\beta}\right)\right)\\
\implies\beta|\sigma^{2},y & \sim N\left(\hat{\beta},\sigma^{2}\left(X^{t}X\right)^{-1}\right)
\end{align*}


\paragraph{4.5.2c.}

Encuentre la distribuci\'{o}n a posterior de $\sigma^{2}$ , (ver
ejercicio 14.4 BDA)
\begin{align*}
p\left(\sigma^{2}|y\right) & =\frac{p\left(\beta,\sigma^{2}|,y\right)}{p\left(\beta|\sigma^{2},y\right)}\\
 & \propto\frac{\left(\sigma^{2}\right)^{-n/2-2}\exp\left(-\frac{1}{2\sigma^{2}}\left(y-X\beta\right)^{t}\left(y-X\beta\right)\right)}{\det\left(\sigma^{2}\left(X^{t}X\right)^{-1}\right)^{-1/2}\exp\left(-\frac{1}{2\sigma^{2}}\left(\beta-\hat{\beta}\right)^{t}X^{t}X\left(\beta-\hat{\beta}\right)\right)}\\
 & =\frac{\left(\sigma^{2}\right)^{-n/2-2}\exp\left(-\frac{1}{2\sigma^{2}}\left(y-X\hat{\beta}\right)^{t}\left(y-X\hat{\beta}\right)-\frac{1}{2\sigma^{2}}\left(\beta-\hat{\beta}\right)^{t}X^{t}X\left(\beta-\hat{\beta}\right)\right)}{\det\left(\sigma^{2}\left(X^{t}X\right)^{-1}\right)^{-1/2}\exp\left(-\frac{1}{2\sigma^{2}}\left(\beta-\hat{\beta}\right)^{t}X^{t}X\left(\beta-\hat{\beta}\right)\right)}\\
 & =\frac{\left(\sigma^{2}\right)^{-n/2-2}\exp\left(-\frac{1}{2\sigma^{2}}\left(y-X\hat{\beta}\right)^{t}\left(y-X\hat{\beta}\right)\right)}{\det\left(\sigma^{2}\left(X^{t}X\right)^{-1}\right)^{-1/2}\exp\left(0\right)}\\
 & =\frac{\left(\sigma^{2}\right)^{-n/2-2}\exp\left(-\frac{1}{2\sigma^{2}}\left(y-X\hat{\beta}\right)^{t}\left(y-X\hat{\beta}\right)\right)}{\left(\sigma^{2k}\right)^{-1/2}\det\left(\left(X^{t}X\right)^{-1}\right)^{-1/2}\exp\left(0\right)}\\
 & \propto\left(\sigma^{2}\right)^{-n/2-2+k/2}\exp\left(-\frac{1}{2\sigma^{2}}\left(y-X\hat{\beta}\right)^{t}\left(y-X\hat{\beta}\right)\right)\\
\implies\sigma^{2}|y & \sim Inv-\chi^{2}\left(n-k,s^{2}\right)
\end{align*}


\paragraph{4.5.2d }

Si $\widetilde{y}$ es una nueva respuesta, existen $a$ y $b$ tales
que 

\section{Ejemplos de c\'{a}tedra}

demuestre que que modelo probit con $\pi\left(x\right)=\Phi\left(\beta_{0}+\beta_{1}x|\mu,\sigma^{2}\right)$
es no identificado para $\mu$ y $\sigma^{2}$. La demostraci\'{o}n
se basa en que 
\begin{align*}
Y_{i}|x_{i} & \sim\text{ber}\left(\pi\left(x_{i}\right)\right)\\
\implies\pi\left(x\right) & =\Phi\left(\beta_{0}+\beta_{1}x|\mu,\sigma^{2}\right)\\
\pi\left(x\right) & =\Phi_{0,1}\left(\frac{\beta_{0}+\beta_{1}x-\mu}{\sigma}\right)\\
\pi\left(x\right) & =\Phi_{0,1}\left(\beta_{0}^{*}+\beta_{1}^{*}x\right)
\end{align*}
entonces existen par\'{a}metros observacionalmente equivalentes.

\paragraph{Intervalo de Confianza }

El par\'{a}metro est\'{a} cubierto por el intervalo de confianza con
una probabilidad $1-\alpha$.

\paragraph*{Identificabilidad bayesiana}

Si tengo un modelo clasicamente no identificado, en bayesiana podemos
\end{document}
