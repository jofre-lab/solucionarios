%% LyX 2.2.4 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsthm}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}
  \theoremstyle{definition}
  \newtheorem{problem}[thm]{\protect\problemname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{pgfplots}
\usepackage{enumitem}
\setenumerate{label=A)}
\setenumerate[2]{label=a)}
% Added by lyx2lyx
\renewcommand{\textendash}{--}
\renewcommand{\textemdash}{---}

\makeatother

\usepackage{babel}
  \providecommand{\problemname}{Problem}
\providecommand{\theoremname}{Theorem}

\begin{document}

\section{Teorema de De Finetti}

Las urnas de Polya son un buen ejemplo de variables aleatorias intercambiables

\section{Primeros ejemplos de medida de De Finetti}

A pesar de que el teorema de representaci\'{o}n de De Finetti nos
garantizan la existencia de una distribuci\'{o}n a priori no nos dice
cual es el precisamente, podemos recolectar algunos ejemplos de ciertas
bibliograf\'{\i}as para hacernos una idea de como operar con estos
conceptos. Si tenemos una muestra aleatoria independiente es simple
ver que
\[
f\left(x_{1},\ldots,x_{n}\right)=\prod_{i=1}^{n}f\left(x_{i}\right)\implies f\left(x_{n+1},\ldots,x_{n+m}|x_{1},\ldots,x_{n}\right)=\prod_{i=n+1}^{m}f\left(x_{i}\right)
\]
, esto quiere decir que no hay aprendizaje desde la experiencia. \marginpar{no hay aprendizaje desde la experiencia} 
\begin{thm}
Sea $\theta\sim f\left(\theta\right)$ y $Y_{1},\ldots,Y_{n}$ condicionalmente
independientes de $\theta$ entonces $Y_{1},\ldots,Y_{n}$ es intercambiable.
\end{thm}

\begin{proof}
Basta con probar lo siguiente 
\begin{align*}
f\left(y_{1},\ldots,y_{n}\right) & =\int f\left(y_{1},\ldots,y_{n}|\theta\right)dF\left(\theta\right)\\
 & =\int\prod_{i=1}^{n}f\left(y_{i}|\theta\right)dF\left(\theta\right)\\
 & =f\left(y_{\pi_{1}},\ldots,y_{\pi_{n}}\right)
\end{align*}
\end{proof}
B\'{a}sicamente el teorema de De Finetti es el rec\'{\i}proco de esta
proposici\'{o}n. La prueba de De Finetti para variables aleatorias
dicot\'{o}micas se explica a continuaci\'{o}n. Consideremos el caso
de una secuencia intercambiable de variables aleatorias $0-1$. Entonces
\begin{align*}
p\left(y_{1},\ldots,y_{n}\right) & =\int\prod_{i=1}^{n}p\left(y_{i}|\theta\right)dF\left(\theta\right)
\end{align*}
donde 
\[
F\left(\theta\right)=\lim_{n\to\infty}P\left(s_{n}\leq\theta\right)
\]
notemos que 
\[
p\left(y_{1}+\ldots+y_{n}=s_{n}\right)=\binom{n}{s_{n}}p\left(y_{1},\ldots,y_{n}\right)=\binom{n}{s_{n}}p\left(y_{\pi_{1}},\ldots,y_{\pi_{n}}\right)
\]
dado que vale para cualquier permutaci\'{o}n, ahora bien, sea $N>n$
entonces 
\[
p\left(y_{1}+\ldots+y_{n}=s_{n}\right)=\sum p\left(y_{1}+\ldots+y_{n}=s_{n}|y_{1}+\ldots+y_{N}=s_{N}\right)p\left(y_{1}+\ldots+y_{N}=s_{N}\right)
\]


\section{BDA 3}

En esta secci\'{o}n se ilustrar\'{a}n algunos conceptos \'{u}tiles
expuestos en el libro de Gelman y algunos ejercicios que son frecuentes
en c\'{a}tedras de inferencia bayesiana que se basan en el texto.

\subsection{Introducci\'{o} y motivaci\'{o}n}

\subsection{Modelos uniparam\'{e}tricos}
\begin{problem}[cap2.prob5]
La distribuci\'{o}n a posterior es un compromiso entre la priori
y los datos: Sea $n$ el n\'{u}mero de lanzamientos de una moneda,
la cual tiene probabilidad de $\theta$ de ser cara. (a) Si la distribuci\'{o}n
a priori es una uniforme entre $0,1$, derive la distribuci\'{o}n
predictiva 
\[
p\left(y=k\right)=\int_{0}^{1}p\left(y=k|\theta\right)d\theta
\]
para cada $k=1,\ldots,n$. Sol: 
\begin{align*}
p\left(y=k\right) & =\int_{0}^{1}p\left(y=k,\theta\right)d\theta\\
 & =\int_{0}^{1}p\left(y=k|\theta\right)\underbrace{p\left(\theta\right)}_{1}d\theta\\
 & =\int_{0}^{1}p\left(y=k|\theta\right)d\theta\\
 & =\int_{0}^{1}\binom{n}{k}\theta^{k}\left(1-\theta\right)^{n-k}d\theta\\
 & =\binom{n}{k}\int_{0}^{1}\theta^{k}\left(1-\theta\right)^{n-k}d\theta\\
 & =\binom{n}{k}\frac{\Gamma\left(k+1\right)\Gamma\left(n-k+1\right)}{\Gamma\left(n+2\right)}\\
 & =\binom{n}{k}\frac{k!\left(n-k\right)!}{\left(n+1\right)!}\\
 & =\frac{1}{n+1}
\end{align*}
usamos el siguiente resultado 
\begin{align*}
\int_{0}^{1}\frac{\Gamma\left(\alpha+\beta\right)}{\Gamma\left(\alpha\right)\Gamma\left(\beta\right)}\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1}d\theta & =1\\
\implies\int_{0}^{1}\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1}d\theta & =\frac{\Gamma\left(\alpha\right)\Gamma\left(\beta\right)}{\Gamma\left(\alpha+\beta\right)}\\
\implies\int_{0}^{1}\theta^{k}\left(1-\theta\right)^{n-k}d\theta & =\frac{\Gamma\left(k+1\right)\Gamma\left(n-k+1\right)}{\Gamma\left(k+1+n-k+1\right)}=\frac{\Gamma\left(k+1\right)\Gamma\left(n-k+1\right)}{\Gamma\left(n+2\right)}
\end{align*}
(b) Supona que asigna una distribuci\'{o}n $\theta\sim\text{beta}\left(\alpha,\beta\right)$
y usted observa $y$ caras en sus $n$ lanzamientos. Muestre algebraicamente
que la media posterior involucra a la media a priori y la media de
los datos. 
\begin{align*}
p\left(\theta|y\right) & \propto p\left(y|\theta\right)p\left(\theta\right)\\
 & =\binom{n}{y}\theta^{y}\left(1-\theta\right)^{n-y}\frac{\Gamma\left(\alpha+\beta\right)}{\Gamma\left(\alpha\right)\Gamma\left(\beta\right)}\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1}\\
 & \propto\theta^{\alpha+y-1}\left(1-\theta\right)^{\beta+n-y-1}\\
\implies\theta|y & \sim\text{beta}\left(\alpha+y,\beta+n-y\right)
\end{align*}
luego, la media condicional 
\begin{align*}
E\left(\theta|y\right) & =\frac{\alpha+y}{\alpha+y+\beta+n-y}\\
 & =\frac{\alpha+y}{\alpha+\beta+n}\\
 & =\frac{\alpha}{\alpha+\beta+n}+\frac{y}{\alpha+\beta+n}\\
 & =\frac{\alpha+\beta}{\alpha+\beta}\frac{\alpha}{\alpha+\beta+n}+\frac{n}{n}\frac{y}{\alpha+\beta+n}\\
 & =\frac{\alpha+\beta}{\alpha+\beta+n}\underbrace{\frac{\alpha}{\alpha+\beta}}_{E\left(\theta\right)}+\frac{n}{\alpha+\beta+n}\frac{y}{n}
\end{align*}
\end{problem}


\paragraph*{2.5c. }

Muestre que si la priori es uniforme entonces la media a posteriori,
entonces la varianza a posterior es siempre menor a la varianza a
priori. Sol: Sea $\theta\sim\text{beta}\left(1,1\right)$

\begin{align*}
\text{Var}\left(\theta|y\right) & =\frac{\left(\alpha+y\right)\left(\beta+n-y\right)}{\left(\alpha+y+\beta+n-y\right)^{2}\left(\alpha+y+\beta+n-y\right)}\\
 & =\frac{\left(\alpha+y\right)\left(\beta+n-y\right)}{\left(\alpha+\beta+n\right)^{2}\left(\alpha+\beta+n+1\right)}\\
 & =\frac{\left(1+y\right)\left(1+n-y\right)}{\left(2+n\right)^{2}\left(2+n+1\right)}=\frac{\left(1+y\right)\left(1+n-y\right)}{\left(2+n\right)^{2}\left(n+3\right)}\leq\frac{1}{12}=\text{Var}\left(\theta\right)
\end{align*}


\paragraph*{2.5d. }

De un ejemplo de una priori $\text{beta}\left(\alpha,\beta\right)$
con $y$ y $n$ donde va varianza posterior es menor a la varianza
a priori. Sol. 
\[
\frac{\left(\alpha+y\right)\left(\beta+n-y\right)}{\left(\alpha+\beta+n\right)^{2}\left(\alpha+\beta+n+1\right)}\leq\frac{\alpha\beta}{\left(\alpha+\beta\right)^{2}\left(\alpha+\beta+1\right)}
\]
y seleccionamos valores que respeten la igualdad.

\paragraph*{2.7.a}

Sobre prioris no informativas: Para la verosimilitud binomial $y\sim\text{bin}\left(n,\theta\right)$
, muestre que $p\left(\theta\right)\propto\theta^{-1}\left(1-\theta\right)^{-1}$
es la priori uniforme para el par\'{a}metro natural de la familia
exponencial. Sol. Sabemos que la funci\'{o}n de enlace can\'{o}nica
de distribuci\'{o}n binomial es $\phi=\log\left(\frac{\theta}{1-\theta}\right)=h\left(\theta\right)$.
sabiendo que 
\begin{align*}
p\left(\theta\right) & =\underbrace{p\left(\phi\right)}_{\propto1}\left|\frac{d\phi}{d\theta}\right|\\
 & \propto\left|\frac{d}{d\theta}\log\left(\frac{\theta}{1-\theta}\right)\right|\\
 & =\frac{1}{\theta}+\frac{1}{1-\theta}\\
 & =\theta^{-1}\left(1-\theta\right)^{-1}
\end{align*}
 .ver BDA (ecuaci\'{o}n 2.19) 

\section{Ayudant\'{\i}a}

Ejercicios de las ayudant\'{\i}as del segundo semestre del 2020

\subsection{Ayudant\'{\i}a 1}

\subsection{Ayudant\'{\i}a 2}

\subsection{Ayudant\'{\i}a 3}

\subsection{Ayudant\'{\i}a 4}

\subsection{Ayudant\'{\i}a 5}

\paragraph{4.5.1a.}

$x_{i}$ permutables $p\left(\mu,\sigma^{2}\right)\propto\left(\sigma^{2}\right)^{-3/2}$.
Buscamos la distribuci\'{o}n a posterior 
\begin{align*}
p\left(\mu,\sigma^{2}|\mathbf{x}\right) & \propto p\left(\mu,\sigma^{2}\right)p\left(\mathbf{x}|\mu,\sigma^{2}\right)\\
 & =\left(\sigma^{2}\right)^{-3/2}\sigma^{-n/2}\exp\left(-\frac{1}{2\sigma^{2}}\sum\left(x_{i}-\mu\right)^{2}\right)\\
 & =\left(\sigma^{2}\right)^{-3/2}\sigma^{-n/2}\exp\left(-\frac{1}{2\sigma^{2}}\sum\left(x_{i}-\overline{x}\right)^{2}+n\left(\overline{x}-\mu\right)^{2}\right)\\
 & =\left(\sigma^{2}\right)^{-3/2}\sigma^{-n/2}\exp\left(-\frac{1}{2\sigma^{2}}\left(\sum\left(x_{i}-\overline{x}\right)^{2}+n\left(\overline{x}-\mu\right)^{2}\right)\right)\\
 & =\left(\sigma^{2}\right)^{-3/2-n/2}\exp\left(-\frac{1}{2\sigma^{2}}\left(\left(n-1\right)s^{2}+n\left(\overline{x}-\mu\right)^{2}\right)\right)\\
\text{kerner de una normal} & \propto\exp\left(-\frac{1}{2\sigma^{2}}\left(n\left(\overline{x}-\mu\right)^{2}\right)\right)\\
\implies\mu|\mathbf{x},\sigma^{2} & \sim N\left(\overline{x},\sigma^{2}/n\right)
\end{align*}


\paragraph{4.5.1b.}

posteriori marginal

\begin{align*}
p\left(\mu,\sigma^{2}|\mathbf{x}\right) & \propto\left(\sigma^{2}\right)^{-3/2-n/2}\exp\left(-\frac{1}{2\sigma^{2}}\left(\left(n-1\right)s^{2}+n\left(\overline{x}-\mu\right)^{2}\right)\right)\\
 & =\left(\sigma^{2}\right)^{-3/2-n/2}\exp\left(-\frac{1}{2\sigma^{2}}\left(n-1\right)s^{2}\right)\underbrace{\exp\left(-\frac{1}{2\sigma^{2}}n\left(\overline{x}-\mu\right)^{2}\right)\left(\frac{\sigma}{n}\right)^{-1/2}}_{\text{kernel de una normal}}\left(\frac{\sigma}{n}\right)^{1/2}\\
\text{integrando}\\
\implies p\left(\sigma^{2}|\mathbf{x}\right) & \propto\left(\sigma^{2}\right)^{-\left(n/2+1\right)}\exp\left(-\frac{1}{2\sigma^{2}}\left(n-1\right)s^{2}\right)\\
\sigma^{2}|\mathbf{x} & \sim\chi^{2}-\mathbf{inv}\left(n,s^{2}\right)
\end{align*}


\paragraph{4.5.1c.}

calcular la distribuci\'{o}n a posteriori marginal de $\mu$
\begin{align*}
p\left(\mu|\mathbf{x}\right) & =\int_{0}^{\infty}p\left(\mu,\sigma^{2}|\mathbf{x}\right)d\sigma^{2}\\
 & \propto\int_{0}^{\infty}\left(\sigma^{2}\right)^{-3/2}\sigma^{-n/2}\exp\left(-\frac{1}{2\sigma^{2}}A\right)d\sigma^{2}\\
A & =\left(\sum\left(x_{i}-\overline{x}\right)^{2}+n\left(\overline{x}-\mu\right)^{2}\right)
\end{align*}
con el siguiente cambio de variable $z=A/\left(2\sigma^{2}\right)$,
$\text{\ensuremath{\sigma^{2}=A/2z} }$ y $d\sigma^{2}=-\frac{A}{2z^{2}}dz$.
Luego 
\begin{align*}
p\left(\mu|\mathbf{x}\right) & \propto\int_{0}^{\infty}\left(\frac{A}{2z}\right)^{-3/2-n/2}\exp\left(-z\right)\left(-\frac{A}{2z^{2}}\right)dz\\
 & \propto\left(\frac{A}{2z}\right)^{-1/2-n/2}
\end{align*}


\paragraph{4.5.1d.}

Utilice los resultados anteriores para encontrar la distribuci\'{o}n
predictiva a posteriori usando el m\'{e}todo de la composici\'{o}n. 
\begin{enumerate}
\item para $s=1\ldots,S$
\item genera $\mu^{\left(s\right)}\sim p\left(\mu|\mathbf{y}\right)$
\item genera $\sigma^{2\left(s\right)}\sim p\left(\sigma^{2}|\mathbf{y}\right)$
\item $y^{s}\sim p\left(x|\mu^{\left(s\right)},\sigma^{2\left(s\right)}\right)$
\end{enumerate}

\paragraph{4.5.1e}

Repita a) y c) con la priori informativa: $p\left(\mu,\sigma^{2}\right)=p\left(\mu|\sigma^{2}\right)p\left(\sigma^{2}\right)$.
Luego 
\begin{align*}
p\left(\mu,\sigma^{2}|\mathbf{x}\right) & \propto p\left(\mu,\sigma^{2}\right)p\left(\mathbf{x}|\mu,\sigma^{2}\right)\\
 & =p\left(\mu|\sigma^{2}\right)p\left(\sigma^{2}\right)p\left(\mathbf{x}|\mu,\sigma^{2}\right)\\
 & =
\end{align*}


\paragraph{4.5.2a.}

Encontre la distribuci\'{o}n a posterior de $\beta,\sigma^{2}$. 
\begin{align*}
y|\beta,\sigma^{2},X & \sim N\left(X\beta,\sigma^{2}I\right)\\
p\left(y|\beta,\sigma^{2},X\right) & \propto\left(\sigma^{2}\right)^{2}\exp\left(-\frac{1}{2\sigma^{2}}\left(y-X\hat{\beta}\right)^{t}\left(y-X\hat{\beta}\right)\right)\exp\left(-\frac{1}{2\sigma^{2}}\left(\beta-\hat{\beta}\right)^{t}X^{t}X\left(\beta-\hat{\beta}\right)\right)\\
\implies
\end{align*}


\paragraph{4.5.2b}

\paragraph{4.5.2d}

\paragraph{4.5.2e}

\section{Ejemplos de c\'{a}tedra}

demuestre que que modelo probit con $\pi\left(x\right)=\Phi\left(\beta_{0}+\beta_{1}x|\mu,\sigma^{2}\right)$
es no identificado para $\mu$ y $\sigma^{2}$. La demostraci\'{o}n
se basa en que 
\begin{align*}
Y_{i}|x_{i} & \sim\text{ber}\left(\pi\left(x_{i}\right)\right)\\
\implies\pi\left(x\right) & =\Phi\left(\beta_{0}+\beta_{1}x|\mu,\sigma^{2}\right)\\
\pi\left(x\right) & =\Phi_{0,1}\left(\frac{\beta_{0}+\beta_{1}x-\mu}{\sigma}\right)\\
\pi\left(x\right) & =\Phi_{0,1}\left(\beta_{0}^{*}+\beta_{1}^{*}x\right)
\end{align*}
entonces existen par\'{a}metros observacionalmente equivalentes.

\paragraph{Intervalo de Confianza }

El par\'{a}metro est\'{a} cubierto por el intervalo de confianza con
una probabilidad $1-\alpha$.

\paragraph*{Identificabilidad bayesiana}

Si tengo un modelo clasicamente no identificado, en bayesiana podemos
\end{document}
